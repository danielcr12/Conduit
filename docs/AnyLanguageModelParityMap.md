# AnyLanguageModel Parity Map (Conduit vs AnyLanguageModel)

Date: 2026-02-12
Baseline AnyLanguageModel path: `/Users/chriskarani/CodingProjects/AnyLanguageModel-main`

Legend:
- Conduit status: `Full` / `Partial` / `Missing` / `Divergent` / `Unverified`
- Risk: `P0` (critical parity/behavior impact), `P1` (high), `P2` (medium/low)

| Area | Feature | Conduit status | AnyLanguageModel behavior (evidence) | Conduit behavior (evidence) | Risk | Recommended fix |
|---|---|---|---|---|---|---|
| Provider surface | OpenAI provider | Full | `OpenAILanguageModel` implements model interface and supports both API variants (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:19`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:458`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:465`) | `OpenAIProvider` implements text generation/streaming (`Sources/Conduit/Providers/OpenAI/OpenAIProvider.swift:72`, `Sources/Conduit/Providers/OpenAI/OpenAIProvider+Streaming.swift:36`) | P1 | Keep parity tests for request/stream behavior as APIs evolve. |
| Provider surface | Anthropic provider | Full | `AnthropicLanguageModel` available (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/AnthropicLanguageModel.swift:30`) | `AnthropicProvider` available (`Sources/Conduit/Providers/Anthropic/AnthropicProvider.swift:109`) | P1 | Maintain mapping tests for stop reasons and tool-call parsing. |
| Provider surface | Gemini native provider | Missing | Native `GeminiLanguageModel` with custom options (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/GeminiLanguageModel.swift:10`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/GeminiLanguageModel.swift:29`) | No Gemini provider type in Conduit provider inventory; only OpenRouter Gemini IDs (`Sources/Conduit/Providers/OpenAI/OpenAIModelID.swift:254`, `Sources/Conduit/Providers/OpenAI/OpenAIModelID.swift:257`) | P0 | Add first-class `GeminiProvider` with request/response/tool mappings. |
| Provider surface | Ollama native API | Partial | Dedicated `OllamaLanguageModel` (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OllamaLanguageModel.swift:16`) | Ollama exposed through OpenAI-compatible endpoint (`Sources/Conduit/Providers/OpenAI/OpenAIEndpoint.swift:58`, `Sources/Conduit/Providers/OpenAI/OllamaConfiguration.swift:43`) | P1 | Decide if Conduit should keep adapter-only approach or add native Ollama protocol path. |
| Provider surface | MLX local runtime | Full | `MLXLanguageModel` uses MLX stack (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/MLXLanguageModel.swift:13`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/MLXLanguageModel.swift:28`) | `MLXProvider` local text runtime (`Sources/Conduit/Providers/MLX/MLXProvider.swift:66`) | P1 | Keep parity on tool/multimodal handling for MLX. |
| Provider surface | CoreML local runtime | Full | Native `CoreMLLanguageModel` with `.mlmodelc` loading (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:21`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:40`) | Conduit now includes first-class CoreML provider surface and model identifier routing (`Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:15`, `Sources/Conduit/Core/Types/ModelIdentifier.swift:62`, `Sources/Conduit/Core/Types/ForwardDeclarations.swift:61`) with trait/dependency/runtime wiring (`Package.swift:35`, `Package.swift:62`, `Package.swift:92`) | P1 | Keep CoreML trait build/test coverage in CI to prevent provider-registry regressions. |
| Provider surface | llama.cpp local runtime | Full | `LlamaLanguageModel` backed by `LlamaSwift` (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:2`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:32`) | `LlamaProvider` backed by `LlamaSwift` (`Package.swift:60`, `Package.swift:86`, `Sources/Conduit/Providers/Llama/LlamaProvider.swift:12`) | P1 | Extend runtime option parity (see llama options rows). |
| Provider surface | Apple system model | Partial | `SystemLanguageModel` provides Apple system runtime (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/SystemLanguageModel.swift:8`) | `FoundationModelsProvider` provides system runtime (`Sources/Conduit/Providers/FoundationModels/FoundationModelsProvider.swift:13`) | P1 | Align tool-bridging behavior and platform coverage semantics. |
| Provider surface | HuggingFace cloud provider | Divergent | No first-class HF text provider type in ALM model list (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/`) | Dedicated `HuggingFaceProvider` (`Sources/Conduit/Providers/HuggingFace/HuggingFaceProvider.swift:95`) | P2 | Keep as intentional Conduit extension; document non-ALM surface. |
| llama.cpp support | Dependency and trait wiring | Full | Trait+dependency+product wiring (`../AnyLanguageModel-main/Package.swift:27`, `../AnyLanguageModel-main/Package.swift:34`, `../AnyLanguageModel-main/Package.swift:68`) | Trait+dependency+product wiring (`Package.swift:40`, `Package.swift:60`, `Package.swift:86`) | P1 | None. |
| llama.cpp support | Model identifier and provider binding | Full | File-path-based model binding (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:191`) | `ModelIdentifier.llama(String)` and provider mapping (`Sources/Conduit/Core/Types/ModelIdentifier.swift:53`, `Sources/Conduit/Core/Types/ModelIdentifier.swift:112`) | P1 | None. |
| llama.cpp support | Context and batch runtime controls | Full | Custom options include context and batch (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:83`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:86`) | `LlamaConfiguration` exposes context/batch (`Sources/Conduit/Providers/Llama/LlamaConfiguration.swift:14`, `Sources/Conduit/Providers/Llama/LlamaConfiguration.swift:19`) and runtime applies them (`Sources/Conduit/Providers/Llama/LlamaProvider.swift:492`, `Sources/Conduit/Providers/Llama/LlamaProvider.swift:493`) | P1 | None. |
| llama.cpp support | Sampling/runtime option depth | Full | Supports repeat penalties + mirostat (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:107`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:137`) | `LlamaConfiguration` now exposes `repeatLastTokens` and `mirostat` (`Sources/Conduit/Providers/Llama/LlamaConfiguration.swift:47`, `Sources/Conduit/Providers/Llama/LlamaConfiguration.swift:53`) and runtime applies penalty window + mirostat samplers (`Sources/Conduit/Providers/Llama/LlamaProvider.swift:533`, `Sources/Conduit/Providers/Llama/LlamaProvider.swift:549`) with configuration tests (`Tests/ConduitTests/Providers/Llama/LlamaProviderTests.swift:45`, `Tests/ConduitTests/Providers/Llama/LlamaProviderTests.swift:64`) | P1 | Keep Llama-trait build/test coverage for sampler wiring regressions. |
| llama.cpp support | Stop sequence and cancellation behavior | Full | Stops on unsupported features and runtime completion paths (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:973`) | Stop sequences + cancellation are handled (`Sources/Conduit/Providers/Llama/LlamaProvider.swift:150`, `Sources/Conduit/Providers/Llama/LlamaProvider.swift:216`, `Sources/Conduit/Providers/Llama/LlamaProvider.swift:831`) | P1 | Keep regression tests for stop-sequence trimming and cancel finish reasons. |
| CoreML support | Package/runtime availability | Full | CoreML trait + transformers dependency (`../AnyLanguageModel-main/Package.swift:25`, `../AnyLanguageModel-main/Package.swift:31`, `../AnyLanguageModel-main/Package.swift:63`) | Conduit now wires `CoreML` trait, swift-transformers dependency/products, and CoreML compile flags (`Package.swift:35`, `Package.swift:62`, `Package.swift:92`, `Package.swift:100`) and adds first-class provider surface (`Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:15`) | P1 | Keep CoreML trait build coverage in CI (`swift build --traits CoreML`). |
| CoreML support | Model loading and validation | Full | Enforces `.mlmodelc`, file existence, compute units (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:47`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:52`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:58`) | CoreML provider validates identifier/path/extension and existence (`Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:175`, `Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:186`, `Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:193`) and loads compiled model with compute units (`Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:248`, `Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:250`, `Sources/Conduit/Providers/CoreML/CoreMLConfiguration.swift:21`) | P1 | Add one live fixture integration test with a real `.mlmodelc` asset for end-to-end decode parity. |
| CoreML support | Runtime controls and tool hooks | Full | Chat template + tools handlers in initializer (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:43`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:44`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:92`) | Conduit now exposes explicit runtime controls for prompt formatting, tool spec strategy, template override, and extra context (`Sources/Conduit/Providers/CoreML/CoreMLConfiguration.swift:47`, `Sources/Conduit/Providers/CoreML/CoreMLConfiguration.swift:56`, `Sources/Conduit/Providers/CoreML/CoreMLConfiguration.swift:77`) and provider-level chat-template/tools handlers with tokenizer chat-template application (`Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:20`, `Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:36`, `Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:273`, `Sources/Conduit/Providers/CoreML/CoreMLProvider.swift:315`) with conversion tests (`Tests/ConduitTests/Providers/CoreML/CoreMLProviderTests.swift:189`, `Tests/ConduitTests/Providers/CoreML/CoreMLProviderTests.swift:238`) | P1 | Keep tokenizer-template and tool-spec conversion tests to lock handler semantics. |
| CoreML support | Coverage depth/tests | Full | Dedicated CoreML test suite with model snapshot download (`../AnyLanguageModel-main/Tests/AnyLanguageModelTests/CoreMLLanguageModelTests.swift:24`, `../AnyLanguageModel-main/Tests/AnyLanguageModelTests/CoreMLLanguageModelTests.swift:33`) | Conduit now includes unit coverage for configuration and chat-template/tool-spec conversion (`Tests/ConduitTests/Providers/CoreML/CoreMLProviderTests.swift:154`, `Tests/ConduitTests/Providers/CoreML/CoreMLProviderTests.swift:189`) plus snapshot-backed integration coverage with Hub download/model discovery and live generation (env-gated) (`Tests/ConduitTests/Providers/CoreML/CoreMLProviderIntegrationTests.swift:23`, `Tests/ConduitTests/Providers/CoreML/CoreMLProviderIntegrationTests.swift:26`, `Tests/ConduitTests/Providers/CoreML/CoreMLProviderIntegrationTests.swift:118`) | P1 | Keep integration suite env-gated and run periodically in release validation. |
| Tool lifecycle | Tool protocol shape | Full | `Tool` protocol surface (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Tool.swift:42`) | Equivalent `Tool` surface (`Sources/Conduit/Core/Protocols/Tool.swift:42`) | P2 | None. |
| Tool lifecycle | Tool output segment priority | Divergent | Structured output checked before string (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Tool.swift:117`) | String checked before structured (`Sources/Conduit/Core/Protocols/Tool.swift:117`) | P2 | Keep documented divergence unless concrete bug appears. |
| Tool lifecycle | Tool schema serialization for provider requests | Full | OpenAI conversion resolves root refs (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:1485`) | OpenAI serialization resolves root refs before JSON schema conversion (`Sources/Conduit/Providers/OpenAI/OpenAIProvider+Helpers.swift:315`) with request-building regression test coverage (`Tests/ConduitTests/Providers/OpenAI/OpenAIProviderRequestBuildingTests.swift:182`) | P1 | Keep regression coverage for nested/root-referenced tool schemas. |
| Tool lifecycle | Tool choice control | Full | Anthropic and OpenAI options include tool-choice behavior (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/AnthropicLanguageModel.swift:533`) | `GenerateConfig.toolChoice` serialized to OpenAI (`Sources/Conduit/Core/Types/GenerateConfig.swift:203`, `Sources/Conduit/Providers/OpenAI/OpenAIProvider+Helpers.swift:161`) | P1 | Keep cross-provider tool-choice mapping tests. |
| Tool lifecycle | Parallel/max tool call controls | Full | Supports `parallel_tool_calls` and `max_tool_calls` (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:298`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:349`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:1064`) | `GenerateConfig` now exposes both `parallelToolCalls` and `maxToolCalls` (`Sources/Conduit/Core/Types/GenerateConfig.swift:209`, `Sources/Conduit/Core/Types/GenerateConfig.swift:215`) and OpenAI request builder emits `max_tool_calls` (`Sources/Conduit/Providers/OpenAI/OpenAIProvider+Helpers.swift:183`) with tests (`Tests/ConduitTests/Providers/OpenAI/OpenAIToolRequestBuildingTests.swift:205`, `Tests/ConduitTests/Providers/OpenAI/OpenAIProviderRequestBuildingTests.swift:206`) | P1 | Keep option-serialization tests for chat and responses variants. |
| Tool lifecycle | Missing tool behavior | Partial | Missing tool emits tool output text and continues (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:1451`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/AnthropicLanguageModel.swift:599`) | `ToolExecutor` now supports policy-based behavior: default throw (`.throwError`) and opt-in non-fatal output (`.emitToolOutput`) (`Sources/Conduit/Core/Tools/ToolExecutor.swift:47`, `Sources/Conduit/Core/Tools/ToolExecutor.swift:149`), covered by tests (`Tests/ConduitTests/Core/ToolExecutorTests.swift:482`) | P1 | Consider plumbing missing-tool policy through higher-level session/provider config for parity-by-default behavior. |
| Tool lifecycle | Transcript tool call/output state | Full | Transcript has tool call/output entries (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Transcript.swift:342`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Transcript.swift:383`) | Equivalent transcript entries (`Sources/Conduit/Core/Types/Transcript.swift:341`, `Sources/Conduit/Core/Types/Transcript.swift:417`) | P2 | None. |
| Tool lifecycle | Tool execution orchestration and loop guard | Partial | Provider-local loops execute tool calls (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/GeminiLanguageModel.swift:259`) | Session-level loop with max rounds (`Sources/Conduit/ChatSession.swift:270`, `Sources/Conduit/ChatSession.swift:550`) | P1 | Keep session-level loop; document behavior contract vs ALM provider-local loops. |
| Tool lifecycle | Tool execution concurrency | Divergent | Provider loops execute sequentially (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:1424`) | `ToolExecutor` batch executes concurrently with ordering (`Sources/Conduit/Core/Tools/ToolExecutor.swift:134`, `Sources/Conduit/Core/Tools/ToolExecutor.swift:155`) | P2 | Keep as intentional performance divergence; ensure deterministic ordering tests stay green. |
| Tool lifecycle | Streaming tool deltas (OpenAI) | Divergent | Responses stream ignores tool call delta events (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:683`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:687`) | Streaming assembles partial/completed tool calls (`Sources/Conduit/Providers/OpenAI/OpenAIProvider+Streaming.swift:279`, `Sources/Conduit/Providers/OpenAI/OpenAIProvider+Streaming.swift:380`) | P2 | Keep richer Conduit behavior; document as superset divergence. |
| Tool lifecycle | Streaming tool deltas (Anthropic) | Divergent | Stream loop only emits text deltas (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/AnthropicLanguageModel.swift:442`) | Parses `input_json_delta` and returns completed tool calls (`Sources/Conduit/Providers/Anthropic/AnthropicProvider+Streaming.swift:589`, `Sources/Conduit/Providers/Anthropic/AnthropicProvider+Streaming.swift:675`) | P2 | Keep richer Conduit behavior; preserve parser hardening. |
| Tool lifecycle | Tool-call retry strategy | Full | Tool-call handlers execute per call; no explicit retry API surfaced in model loops (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:1424`) | Conduit now defines explicit retry policy semantics (`Sources/Conduit/Core/Tools/ToolExecutor.swift:56`), applies policy in `ChatSession` tool loops (`Sources/Conduit/ChatSession.swift:270`, `Sources/Conduit/ChatSession.swift:581`), and covers no-retry + retry-success behavior in tests (`Tests/ConduitTests/Core/ToolExecutorTests.swift:583`, `Tests/ConduitTests/ChatSessionTests.swift:673`) | P1 | Keep retry-policy behavior tests to prevent silent semantic regressions. |
| Streaming semantics | Stream abstraction and collect semantics | Partial | `ResponseStream.collect()` returns final response from last snapshot (`../AnyLanguageModel-main/Sources/AnyLanguageModel/LanguageModelSession.swift:837`) | `StreamingResult.collect()` returns final parsed value from last snapshot (`Sources/Conduit/Core/Streaming/StreamingResult.swift:92`) | P2 | Keep equivalent semantics; ensure no-content behavior is documented (`StreamingError.noContent`). |
| Streaming semantics | Finish reason taxonomy | Divergent | Finish reason mostly provider-level string parsing (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:879`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/AnthropicLanguageModel.swift:800`) | Unified cross-provider `FinishReason` enum including tool/cancel/context-window states (`Sources/Conduit/Core/Types/FinishReason.swift:40`) | P2 | Keep Conduit normalized finish-reason model. |
| Streaming semantics | Stop sequence handling | Full | Anthropic/OpenAI options include stop sequences (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/AnthropicLanguageModel.swift:71`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:804`) | `GenerateConfig.stopSequences` passed through providers and llama runtime (`Sources/Conduit/Core/Types/GenerateConfig.swift:270`, `Sources/Conduit/Providers/Llama/LlamaProvider.swift:150`) | P1 | Continue provider-specific stop-sequence tests. |
| Streaming semantics | Cancellation behavior | Full | Stream termination cancels task in model streams (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:701`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/AnthropicLanguageModel.swift:465`) | Cancellation propagated and tested (`Sources/Conduit/ChatSession.swift:721`, `Tests/ConduitTests/Core/StreamingCancellationTests.swift:129`) | P1 | None. |
| Streaming semantics | Chunk richness (tool/reasoning/usage) | Divergent | Snapshot stream carries partial content + raw content (`../AnyLanguageModel-main/Sources/AnyLanguageModel/LanguageModelSession.swift:779`) | `GenerationChunk` includes usage, partial tool calls, completed tool calls, reasoning details (`Sources/Conduit/Core/Streaming/GenerationChunk.swift:170`) | P2 | Keep Conduit superset chunk model. |
| Streaming semantics | OpenAI Responses streaming support | Full | Responses stream path implemented (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:643`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:652`) | OpenAI provider now routes `.responses` streaming and decodes `response.output_text.delta`, `response.tool_call.*`, and `response.completed` events (`Sources/Conduit/Providers/OpenAI/OpenAIProvider+Streaming.swift:129`, `Sources/Conduit/Providers/OpenAI/OpenAIProvider+Streaming.swift:490`, `Sources/Conduit/Providers/OpenAI/OpenAIProvider+Streaming.swift:579`) with decoding tests (`Tests/ConduitTests/Providers/OpenAI/OpenAIAPIVariantTests.swift:167`) | P1 | Add end-to-end SSE fixture tests for additional event permutations from OpenAI/OpenRouter. |
| Structured output | Core schema and dynamic schema API | Full | `GenerationSchema` and `DynamicGenerationSchema` exposed (`../AnyLanguageModel-main/Sources/AnyLanguageModel/GenerationSchema.swift:11`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/DynamicGenerationSchema.swift:8`) | Equivalent schema APIs exposed (`Sources/Conduit/Core/Types/GenerationSchema.swift:30`, `Sources/Conduit/Core/Types/DynamicGenerationSchema.swift:8`) | P1 | None. |
| Structured output | Structured generation entrypoints | Full | `respond(...generating:)` and stream with typed output (`../AnyLanguageModel-main/Sources/AnyLanguageModel/LanguageModelSession.swift:170`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/LanguageModelSession.swift:222`) | `TextGenerator.generate(returning:)` and `stream(returning:)` (`Sources/Conduit/Providers/Extensions/TextGenerator+StructuredOutput.swift:42`, `Sources/Conduit/Providers/Extensions/TextGenerator+StructuredStreaming.swift:45`) | P1 | Keep typed API parity tests. |
| Structured output | Partial JSON/repair streaming resilience | Partial | Uses generated-content snapshots from accumulated text (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:678`) | Explicit strict + partial decode + repair fallback (`Sources/Conduit/Providers/Extensions/TextGenerator+StructuredStreaming.swift:82`) | P2 | Keep Conduit stronger resilience behavior. |
| Structured output | Provider-native schema enforcement vs prompt-instruction | Partial | Native provider APIs used when available (e.g., OpenAI response format/schema paths in request builders) (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:1024`) | Generic helper appends schema-in-prompt and parses output (`Sources/Conduit/Providers/Extensions/TextGenerator+StructuredOutput.swift:48`) | P1 | Expand native response-format support per provider where available. |
| Multimodal | Transcript image segment abstraction | Full | `Transcript.ImageSegment` supports data/URL (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Transcript.swift:114`) | Equivalent `Transcript.ImageSegment` (`Sources/Conduit/Core/Types/Transcript.swift:114`) and `Message.ContentPart` with image/audio (`Sources/Conduit/Core/Types/Message.swift:423`) | P1 | None. |
| Multimodal | OpenAI image block conversion | Full | Converts image segments to OpenAI blocks (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:1229`) | Converts message parts to OpenAI image/audio formats (`Sources/Conduit/Providers/OpenAI/OpenAIProvider+Helpers.swift:290`) | P1 | Maintain multimodal request-build tests. |
| Multimodal | Anthropic image conversion | Full | Converts image to Anthropic blocks (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/AnthropicLanguageModel.swift:734`) | Anthropic API block model supports image/tool/text block handling (`Sources/Conduit/Providers/Anthropic/AnthropicAPITypes.swift:847`) | P1 | Keep regression tests around image block serialization. |
| Routing/capability detection | Unified provider routing metadata | Divergent | Routing is implicit by concrete model type implementing `LanguageModel` (`../AnyLanguageModel-main/Sources/AnyLanguageModel/LanguageModel.swift:3`) | Explicit `ProviderType` and modelâ†’provider mapping (`Sources/Conduit/Core/Types/ForwardDeclarations.swift:49`, `Sources/Conduit/Core/Types/ModelIdentifier.swift:107`) | P2 | Keep Conduit explicit routing model; document divergence from ALM style. |
| Routing/capability detection | Availability model | Divergent | `Availability<UnavailableReason>` on each model (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Availability.swift:2`) | `isAvailable` + `ProviderAvailability` on providers (`Sources/Conduit/Core/Protocols/AIProvider.swift:161`, `Sources/Conduit/Core/Types/ForwardDeclarations.swift:168`) | P2 | Keep Conduit provider-centric availability contract. |
| Routing/capability detection | Capability descriptors and architecture typing | Divergent | Capability checks mostly model-specific; no central architecture registry in base API (`../AnyLanguageModel-main/Sources/AnyLanguageModel/LanguageModel.swift:3`) | `ModelCapabilities` + `ArchitectureType` centralized (`Sources/Conduit/Core/Types/ModelCapabilities.swift:51`, `Sources/Conduit/Core/Types/ModelCapabilities.swift:78`) | P2 | Keep centralized capability surface. |
| Routing/capability detection | VLM detection service | Divergent | No dedicated VLM detector service surfaced in ALM sources | Layered `VLMDetector` service (`Sources/Conduit/Services/VLMDetector.swift:13`) | P2 | Keep Conduit detection system; optionally expose similar utility facade for API parity. |
| Routing/capability detection | MLX compatibility checker | Divergent | No dedicated MLX compatibility checker surfaced in ALM sources | `MLXCompatibilityChecker` validates repo compatibility (`Sources/Conduit/Services/MLXCompatibilityChecker.swift:86`) | P2 | Keep Conduit compatibility checker as value-add divergence. |
| Model management/downloading | Central model manager and cache | Divergent | Per-model loading paths (e.g., MLX load by hub/directory) (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/MLXLanguageModel.swift:67`) | `ModelManager` actor with cache/lifecycle APIs (`Sources/Conduit/ModelManagement/ModelManager.swift:81`) | P2 | Keep as intentional Conduit extension; align docs for ALM users. |
| Model management/downloading | Hugging Face snapshot downloader | Divergent | Tests invoke Hub snapshot directly in model tests (`../AnyLanguageModel-main/Tests/AnyLanguageModelTests/CoreMLLanguageModelTests.swift:33`) | Dedicated downloader actor (`Sources/Conduit/ModelManagement/HuggingFaceHubDownloader.swift:24`) | P2 | Keep dedicated downloader; add parity docs for migration users. |
| Model management/downloading | Diffusion model management | Divergent | No image diffusion model manager in ALM sources | `DiffusionModelRegistry` + `DiffusionModelDownloader` (`Sources/Conduit/ImageGeneration/DiffusionModelRegistry.swift:30`, `Sources/Conduit/ImageGeneration/DiffusionModelDownloader.swift:31`) | P2 | Keep as Conduit-only extension. |
| Error/compat behavior | Unified error taxonomy | Divergent | Session-level generation/tool errors (`../AnyLanguageModel-main/Sources/AnyLanguageModel/LanguageModelSession.swift:671`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/LanguageModelSession.swift:746`) | Unified cross-provider `AIError` taxonomy (`Sources/Conduit/Core/Errors/AIError.swift:30`) | P2 | Keep `AIError` model; maintain provider-specific mapping tests. |
| Error/compat behavior | Unsupported feature mapping in local runtimes | Full | Local runtime specific errors include unsupported feature (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/LlamaLanguageModel.swift:1249`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/CoreMLLanguageModel.swift:244`) | Local runtime errors mapped via `AIError.unsupported*` variants (`Sources/Conduit/Core/Errors/AIError.swift:83`, `Sources/Conduit/Core/Errors/AIError.swift:171`) | P1 | Continue mapping consistency tests. |
| Error/compat behavior | OpenAI Responses compatibility behavior | Full | Supports chat + responses API variants (`../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:28`, `../AnyLanguageModel-main/Sources/AnyLanguageModel/Models/OpenAILanguageModel.swift:35`) | Conduit now executes both variants: runtime request routing + variant-aware non-stream parsing and Responses request/tool serialization (`Sources/Conduit/Providers/OpenAI/OpenAIProvider+Helpers.swift:29`, `Sources/Conduit/Providers/OpenAI/OpenAIProvider+Helpers.swift:223`, `Sources/Conduit/Providers/OpenAI/OpenAIProvider+Helpers.swift:587`) with parser/request tests (`Tests/ConduitTests/Providers/OpenAI/OpenAIAPIVariantTests.swift:67`, `Tests/ConduitTests/Providers/OpenAI/OpenAIAPIVariantTests.swift:102`) | P1 | Keep compatibility fixtures for evolving Responses payload fields. |
| Error/compat behavior | Retry configuration surface | Divergent | No central retry config type surfaced in ALM base API | OpenAI retry config is first-class (`Sources/Conduit/Providers/OpenAI/RetryConfiguration.swift:54`) | P2 | Keep as Conduit extension; optionally add ALM-style custom option bridging docs. |

## Coverage Notes
- All user-requested feature families are covered in the table: provider surface, llama.cpp, CoreML, tool lifecycle, streaming semantics, structured outputs, routing/capabilities, model management/downloading, and error/finish compatibility.
- No remaining `Unverified` rows in this parity map revision.
